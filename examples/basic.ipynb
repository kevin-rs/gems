{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29daa5b4",
   "metadata": {},
   "source": [
    "# Google Colab Rust Setup\n",
    "\n",
    "The following cell is used to set up a Rust environment on Colab. Don't execute it locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37160ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script sets up and spins up a Jupyter Notebook environment with a Rust kernel using Nix and IPC Proxy. \n",
    "!wget -qO- https://gist.github.com/wiseaidev/2af6bef753d48565d11bcd478728c979/archive/3f6df40db09f3517ade41997b541b81f0976c12e.tar.gz | tar xvz --strip-components=1\n",
    "!bash setup_evcxr_kernel.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe4adc",
   "metadata": {},
   "source": [
    "# Gems Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ffded",
   "metadata": {},
   "source": [
    "This Jupyter Notebook provides a guide on using the [**`gems`**](https://github.com/wiseaidev/gems) cli and SDK in Rust to interact with the Gemini API. The [**Gemini API**](https://ai.google.dev/docs) allows you to generate creative content using large language models. In this notebook, we'll cover various aspects of using Gemini, including text-only input, text-and-image input, multi-turn conversations, configuration options, streaming content generation, counting tokens, embedding, and model information.\n",
    "\n",
    "Let's start by installing the sdk to run the code snippets in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9d82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    ":dep gems = { version = \"0.1.0\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccd096",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd826d00",
   "metadata": {},
   "source": [
    "## Import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953b8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "use gems::Client;\n",
    "use gems::traits::CTrait;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531035ba",
   "metadata": {},
   "source": [
    "## API Key\n",
    "\n",
    "To use the Gemini API, you'll need an API key. If you don't already have one, create a key in [Google AI Studio](https://makersuite.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aca06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn config() -> (String, String) {\n",
    "    let api_key = \"your-api-key\";\n",
    "    let model = \"gemini-2.0-flash\";\n",
    "    (api_key.to_string(), model.to_string())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472a027",
   "metadata": {},
   "source": [
    "## 1. Text-Only Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb2b6a",
   "metadata": {},
   "source": [
    "To generate content using only text input, you can use the **`chat`** method with the `gemini-2.0-flash` model. Replace `Your_API_Key` above with your actual API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a01cc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Subprocess terminated with status: signal: 9 (SIGKILL)",
     "output_type": "error",
     "traceback": [
      "Subprocess terminated with status: signal: 9 (SIGKILL)"
     ]
    }
   ],
   "source": [
    "use gems::messages::Content;\n",
    "use gems::messages::Message;\n",
    "use gems::models::Model;\n",
    "use gems::chat::ChatBuilder;\n",
    "\n",
    "\n",
    "let mut gemini_client = Client::builder().model(&config().1).build()?;\n",
    "\n",
    "gemini_client.set_api_key(config().0);\n",
    "\n",
    "let parameters = ChatBuilder::default()\n",
    "    .model(Model::Flash20)\n",
    "    .messages(vec![Message::User {\n",
    "        content: Content::Text(\"Hello\".to_string()),\n",
    "        name: None,\n",
    "    }])\n",
    "    .build()?;\n",
    "\n",
    "match gemini_client.chat().generate(parameters).await {\n",
    "    Ok(response) => {\n",
    "        println!(\"{}\", response);\n",
    "    }\n",
    "    Err(err) => {\n",
    "        eprintln!(\"Error: {:?}\", err);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d54b9",
   "metadata": {},
   "source": [
    "## 2. Streaming Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7cea9",
   "metadata": {},
   "source": [
    "By default, the model returns a response after completing the entire generation process. You can achieve faster interactions by not waiting for the entire result and instead use streaming to handle partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bab3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "use gems::stream::StreamBuilder\n",
    "use gems::messages::Content;\n",
    "use gems::messages::Message;\n",
    "\n",
    "let input_text = \"Write a story about a magic backpack.\";\n",
    "            \n",
    "let parameters = StreamBuilder::default()\n",
    "    .model(Model::Flash20)\n",
    "    .input(Message::User {\n",
    "        content: Content::Text(input_text.to_string()),\n",
    "        name: None,\n",
    "    })\n",
    "    .build()?;\n",
    "\n",
    "match gemini_client.stream().generate(parameters).await {\n",
    "    Ok(response) => {\n",
    "        println!(\"{:?}\", response);\n",
    "    }\n",
    "    Err(err) => {\n",
    "        eprintln!(\"Error: {:?}\", err);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e6d89",
   "metadata": {},
   "source": [
    "## 3. Counting Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd077c8",
   "metadata": {},
   "source": [
    "When using long prompts, it might be useful to count tokens before sending any content to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "use gems::tokens::TokenBuilder\n",
    "use gems::messages::Content;\n",
    "use gems::messages::Message;\n",
    "\n",
    "let input_text = \"Write a story about a magic backpack.\";\n",
    "        \n",
    "let params = TokenBuilder::default()\n",
    "    .input(Message::User {\n",
    "        content: Content::Text(input_text.to_string()),\n",
    "        name: None,\n",
    "    })\n",
    "    .build()?;\n",
    "\n",
    "let count = gemini_client.tokens().count(params).await?;\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef1199",
   "metadata": {},
   "source": [
    "## 4. Embedding\n",
    "\n",
    "Embedding is a technique used to represent information as a list of floating point numbers in an array. With `gems`, you can represent text in a vectorized form. The following code demonstrates embedding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c20478",
   "metadata": {},
   "outputs": [],
   "source": [
    "use gems::embed::EmbeddingBuilder;\n",
    "use gems::models::Model;\n",
    "\n",
    "let input_text = \"Hello\";\n",
    "        \n",
    "let params = EmbeddingBuilder::default()\n",
    "    .model(Model::Embedding)\n",
    "    .input(Message::User {\n",
    "        content: Content::Text(input_text.to_string()),\n",
    "        name: None,\n",
    "    })\n",
    "    .build()?;\n",
    "\n",
    "gemini_client.set_model(Model::Embedding);\n",
    "let response = gemini_client.embeddings().create(params).await?;\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5060cd",
   "metadata": {},
   "source": [
    "## 5. Model Information\n",
    "\n",
    "Your can retrieve information about the current model being used by the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19376863",
   "metadata": {},
   "outputs": [],
   "source": [
    "use gems::models::Model;\n",
    "use gems::models::ModBuilder;\n",
    "\n",
    "let params = ModBuilder::default().model(Model::default()).build()?;\n",
    "let model_info = gemini_client.models().get(params).await?;\n",
    "\n",
    "model_info.print();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bba94d",
   "metadata": {},
   "source": [
    "## 6. Models Information\n",
    "\n",
    "You can retrieve information about all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8804b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "let models = gemini_client.models().list().await?;\n",
    "\n",
    "models.print();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c382a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de259a",
   "metadata": {},
   "source": [
    "## Gems CLI\n",
    "### Shell Command Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b71d5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use std::process::{Command, Output, Stdio};\n",
    "\n",
    "// A helper function to execute a shell command from a Rust script\n",
    "fn execute_command(command: &str) -> Result<(), std::io::Error> {\n",
    "    let status = Command::new(\"bash\")\n",
    "        .arg(\"-c\")\n",
    "        .arg(command)\n",
    "        .stderr(Stdio::inherit())\n",
    "        .status()?;\n",
    "\n",
    "    if status.success() {\n",
    "        Ok(())\n",
    "    } else {\n",
    "        Err(std::io::Error::from_raw_os_error(status.code().unwrap_or(1)))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a53a882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gems 0.0.9\n",
      "\n",
      "💎 GEMS\n",
      "=======\n",
      "\n",
      "A command-line tool for interacting with the Google Gemini API.\n",
      "\n",
      "FUNCTIONALITIES:\n",
      "  - Generate Content: Generate content using the Gemini API.\n",
      "  - Stream Generate Content: Stream content generation using the Gemini API.\n",
      "  - Count Tokens: Count tokens in a given text using the Gemini API.\n",
      "  - Embed Content: Embed content using the Gemini API.\n",
      "  - Batch Embed Contents: Batch embed contents using the Gemini API.\n",
      "  - Get Model Info: Get information about the model from the Gemini API.\n",
      "  - List Models: List available models from the Gemini API.\n",
      "\n",
      "USAGE:\n",
      "  gems [OPTIONS] <COMMAND>\n",
      "\n",
      "EXAMPLES:\n",
      "  Generate content:\n",
      "    gems generate -t \"Hello\"\n",
      "\n",
      "  Stream generate content:\n",
      "    gems stream -t \"Generate a short fictional story\"\n",
      "\n",
      "  Vision:\n",
      "    gems vision -i /path/to/your/image.jpg\n",
      "\n",
      "  Count tokens:\n",
      "    gems count -t \"Hello There!\"\n",
      "\n",
      "  Embed content:\n",
      "    gems -m 'embedding-001' embed -t \"Write a story about a magic backpack.\"\n",
      "\n",
      "  Batch embed contents:\n",
      "    gems -m 'embedding-001' batch -t \"Write a story about a magic backpack.\",\"Generate a poem about nature.\"\n",
      "\n",
      "  Get model info:\n",
      "    gems info\n",
      "\n",
      "  List models:\n",
      "    gems list\n",
      "\n",
      "For more information, visit: github.com/wiseaidev/gems\n",
      "\n",
      "\n",
      "\n",
      "Usage: gems [OPTIONS] <COMMAND>\n",
      "\n",
      "Commands:\n",
      "  generate  \n",
      "  vision    \n",
      "  stream    \n",
      "  count     \n",
      "  embed     \n",
      "  batch     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  info      \n",
      "  list      \n",
      "  help      Print this message or the help of the given subcommand(s)\n",
      "\n",
      "Options:\n",
      "  -a, --api-key <API_KEY>  API key for authentication\n",
      "  -m, --model <MODEL>      Model to be used\n",
      "  -h, --help               Print help\n",
      "  -V, --version            Print version\n",
      "\n",
      "AUTHORS:\n",
      "    Mahmoud Harmouch\n"
     ]
    }
   ],
   "source": [
    "let command = \"../target/release/gems --help\";\n",
    "\n",
    "if let Err(err) = execute_command(command) {\n",
    "    eprintln!(\"Error executing command: {}\", err);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128dd07",
   "metadata": {},
   "source": [
    "## 1. Text-Only Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6748c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let command = \"GEMINI_API_KEY=Your_API_Key ../target/release/gems generate -t hello\";\n",
    "\n",
    "if let Err(err) = execute_command(command) {\n",
    "    eprintln!(\"Error executing command: {}\", err);\n",
    "}\n",
    "\n",
    "// Replace Your_API_Key above with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418a25f",
   "metadata": {},
   "source": [
    "## 2. Streaming Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120d257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chipped porcelain teacup trembled in Elara’s hand, mirroring the anxiety churning in her stomach. Outside, the perpetual rain of Aethelgard hammered against the stained-glass windows of the sanctuary, a melancholic percussion accompanying her confession.\n",
      "\n",
      "“It started subtly, Reverend,” she whispered, her voice barely audible above the storm. “With the shadows. Just a flicker in the corner of my eye, a fleeting sense of…being watched.”\n",
      "\n",
      "Reverend Silas, a man whose wrinkles held the weight of countless confessions, simply nodded, his gaze fixed on the intricate carvings of the altar.\n",
      "\n",
      "“Then, the dreams. Vivid, unsettling. I walk through the whispering woods, always searching for something…or someone. And the trees…their branches writhe like grasping fingers.” Elara shivered, pulling her shawl tighter.\n",
      "\n",
      "For generations, the women of Aethelgard had been blessed – or cursed – with the Sight. Visions, prophecies, snippets of the future. But Elara's gift felt different, darker.\n",
      "\n",
      "s expression flickered.  stretched, thick and heavy. Then, Silas leaned forward, his eyes suddenly sharp. “Did you…recognize him?”\n",
      "\n",
      "Elara shook her head, tears blurring her vision. “No. Just…a feeling. A sense of impending…doom.”\n",
      "\n",
      "The next morning, the rain had finally stopped. A fragile ray of sunlight pierced through the clouds, illuminating the town square. Elara, compelled by an unknown force, found herself standing before the sealed well.\n",
      "\n",
      "Around her, villagers gathered, murmuring in hushed tones. The seal, a thick slab of granite, had been cracked open. And inside, staring up at the pale sky, was the body of a young man.\n",
      "\n",
      "His hair was the color of raven's wings.\n",
      "\n",
      "Elara gasped, a scream building in her throat. She had seen him. She had seen his death. But she hadn't seen his killer. And now, the weight of that knowledge, of that prophecy, settled on her shoulders, heavy and suffocating, a chilling promise of more darkness to come. The Sight, she realized, was not a gift. It was a curse. And Aethelgard, bathed in the deceptive light of the morning sun, was about to be swallowed whole.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let command = \"GEMINI_API_KEY=Your_API_Key ../target/release/gems stream -t 'Generate a short fictional story'\";\n",
    "\n",
    "if let Err(err) = execute_command(command) {\n",
    "    eprintln!(\"Error executing command: {}\", err);\n",
    "}\n",
    "\n",
    "// Replace Your_API_Key above with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6630a5",
   "metadata": {},
   "source": [
    "## 3. Counting Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a870cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let command = \"GEMINI_API_KEY=Your_API_Key ../target/release/gems count -t 'Hello There!'\";\n",
    "\n",
    "if let Err(err) = execute_command(command) {\n",
    "    eprintln!(\"Error executing command: {}\", err);\n",
    "}\n",
    "\n",
    "// Replace Your_API_Key above with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3f1b9",
   "metadata": {},
   "source": [
    "## 4. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d9b2a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed Content: EmbedContentResponse { embedding: Some(Embedding { values: [0.048244964, 0.011776606, -0.0115520675, -0.018164538, -0.002611024, 0.050926752, 0.08172899, 0.007869764, 0.054475933, 0.026131338, -0.06593486, -0.0022568672, 0.03878191, 0.010530227, -0.04230794, -0.038269036, -0.0098457895, -0.00757351, -0.08022082, -0.03147243, 0.0022871997, -0.0012856963, 0.006200954, -0.03950943, -0.01869138, 0.045126893, 0.004105777, -0.0033850763, 0.023997491, -0.012320019, -0.03423185, 0.04703393, -0.022898493, -0.047150094, 0.00780749, 0.01974024, -0.043441188, 0.025867185, 0.0030507015, -0.08875779, -0.025923807, 0.06206684, 0.014752337, -0.0058687106, 0.020233197, 0.0046556373, 0.09399772, 0.045711495, -0.003133404, 0.017434591, 0.05630074, 0.011448362, -0.07785574, 0.010087488, 0.02003082, 0.015717002, -0.007205217, -0.030741267, 0.12710878, 0.0029991518, -0.032721598, -0.033147562, 0.02915778, -0.0074991942, 0.003163215, -0.015940316, 0.004189512, 0.015689038, -0.028875796, -0.019337106, -0.036190014, 0.016280707, -0.016069036, 0.02213417, -0.053916402, -0.003003477, -0.031128712, 0.018310294, 0.00091360055, 0.0072671706, -0.0126767205, 0.03380801, -0.011924756, 0.039181523, -0.032927774, 0.0110850865, 0.018564412, -0.0043865624, -0.05523017, 0.035607375, 0.040584672, -0.02230653, -0.020282805, 0.034746185, 0.021681188, -0.007846198, -0.06770021, -0.0240012, 0.05561755, 0.03643306, 0.008443081, -0.022553708, 0.0063677877, -0.04451104, 0.008210065, 0.02924832, -0.073751464, -0.009335704, -0.03599839, -0.016313551, 0.069549955, -0.044394, 0.022489144, -0.01829816, -0.0069123264, -0.063189745, -0.083239555, 0.05419444, -0.059869017, 0.007848112, 0.01806428, -0.00016974032, -0.013990147, 0.059604313, 0.017377019, -0.03978634, 0.014663165, -0.02725667, -0.05040479, -0.04766727, 0.077616535, -0.060624022, -0.0322626, 0.0066614454, -0.056755096, -0.0346207, 0.050105907, -0.02176396, 0.009608746, -0.020667186, 0.031436656, -0.012343895, -0.02584352, 0.0052905725, 0.015358834, -0.036663447, -0.028805818, 0.029326372, -0.03675607, 0.0057523013, 0.008646992, 0.003271318, 0.036886215, -0.06496139, -0.0017764379, 0.06711319, 0.05403917, -0.07676129, 0.07018711, 0.0032836234, -0.025738187, -0.06484696, -0.017888471, -0.013525196, -0.017856712, 0.0052227713, -0.009556284, -0.012144731, -0.008154874, -0.010050165, -0.030161574, -0.02687636, 0.029946446, 0.010491553, -0.035400443, -0.03847312, -0.043523084, -0.04146596, -0.014904876, -0.05612963, 0.09271192, -0.010179558, -0.021872884, -0.06701292, 0.009300666, 0.013880859, 0.006405908, 0.009870782, 0.055445455, 0.057509698, -0.011335902, 0.037037913, -0.020233564, 0.013495987, -0.024146711, 0.028285341, 0.09232015, -0.037723426, 0.02304295, 0.00070181076, 0.008637694, 0.055471033, -0.006396016, 0.029328752, -0.03127841, 0.054902516, -0.06906876, -0.070626974, 0.03834553, -0.0034667694, 0.011490643, 0.014109662, -0.0035012485, -0.040147766, -0.011808833, -0.0079955775, 0.094604395, -0.0073381145, 0.028905338, -0.02024914, 0.024386832, 0.066233315, -0.0086844675, 0.038629245, 0.05549848, 0.047197986, -0.04032669, -0.010360459, -0.043882523, -0.031587478, -0.037045117, -0.01387335, -0.053797122, 0.08053998, 0.013922487, 0.035225756, 0.08233647, 0.013619776, 0.04205973, 0.039090168, -0.036261242, -0.064337924, 0.06566691, 0.021861464, -0.014596388, -0.041367393, 0.00586916, 0.0223739, 0.04884985, -0.03674706, -0.037812475, 0.0019823695, -0.066930555, 0.009043339, -0.06007277, -0.043423574, 0.036187157, -0.05175631, -0.019801628, 0.035296306, 0.06410759, -0.051305972, 0.0251233, -0.121552505, -0.008426941, -0.045555603, 0.0131653715, -0.013080472, 0.019376287, -0.087202676, 0.013603416, 0.00073007355, -0.00800969, -0.0008202473, -0.025963739, -0.031919066, -0.029372087, 0.023004537, -0.05583778, 0.015673056, -0.017262997, -0.0031232606, 0.05995642, 0.023636667, -0.0675869, -0.02019377, 0.04567469, 0.028325351, -0.03493539, 0.01833527, 0.017514788, 0.015524491, -0.008267263, 0.008066144, 0.031037187, -0.008633532, 0.024989156, 0.032000992, 0.024004916, 0.018264282, 0.00316655, 0.0581254, -0.050621636, -0.017454369, -0.033102166, -0.007390623, -0.01229711, -0.024594517, -0.0018731147, -0.013322024, -0.048991002, 0.038610764, 0.008227331, -0.008452334, -0.033409137, -5.8596484e-5, -0.09592359, -0.011657979, -0.044698782, -0.04247955, 0.018939065, 0.0036658584, -0.08137263, -0.03160506, -0.0034047414, -0.025208063, 0.029929454, 0.057112005, 0.01013141, 0.008331662, 0.029679067, 0.019469697, 0.02200113, -0.05360101, 0.032528758, 0.06544426, -0.048904143, 0.027643012, 0.047894187, 0.031547964, 0.006553406, 0.0605739, 0.07070022, 0.020473853, 0.0028788762, -0.041780043, 0.003369028, 0.009384539, 0.005092924, -0.094420604, 0.004873564, 0.033435684, 0.019581808, -0.038260102, -0.07577422, 0.009509646, 0.067873545, -0.011958926, 0.02612823, 0.0022701328, 0.022538798, 0.0484252, 0.0010301949, 0.000580448, -0.031301994, 0.03202171, 0.034520518, 0.054580692, 0.012458192, -0.04210212, 0.0053207274, 0.054393165, -0.036675643, -0.02916375, 0.01855962, -0.06473506, -0.051451698, 0.010864846, 0.01166319, -0.07509663, -0.03147292, 0.015180981, -0.05256864, -0.017325174, -0.045264386, 0.101022504, -0.047401458, 0.027169315, -0.002983032, 0.011900212, -0.00041279767, 0.060146432, 0.00813345, 0.019617947, 0.025500758, -0.004804454, -0.038624883, 0.011491707, 0.024300858, 0.009853024, 0.029076295, -0.0013395309, 0.071107805, -0.009655367, -0.022013143, -0.0397018, 0.033526137, 0.029202418, 0.021418823, -0.01649153, -0.019986415, 0.012398917, 0.01993846, 0.0148455445, -0.050171953, -0.03957468, -0.00010025723, 0.0149938455, -0.03958791, -0.007866639, -0.04506342, -0.0053082434, 0.030479986, -0.044793624, 0.005211181, -0.0066887173, -0.007232421, -0.0073843887, 0.034267634, -0.04365864, -0.0019773415, 0.015672844, -0.037999585, -0.017133411, -0.029742587, 0.00041569967, 0.018379701, 0.025408126, 0.0004235032, 0.03464696, -0.05602391, -0.026162213, 0.029715871, 0.029550407, -0.056612186, 0.06881436, 0.030752193, -0.027075997, -0.0063202353, -0.014317209, 0.018972514, -0.01832482, -0.026288668, 0.005298353, -0.06353879, -0.008605906, 0.00960477, 0.032940775, 0.012162132, 0.019159602, -0.07192239, 0.0017953536, 0.08283539, -0.048867237, -0.027887894, -0.012899636, -0.022533417, -0.000197941, -0.031098979, 0.01033841, 0.038314845, 0.029540665, 0.018920388, -0.01613999, 0.033741176, -0.0054729213, -0.034920815, 0.054326184, -0.01856138, 0.041834522, -0.015535919, -0.033234745, -0.01712079, 0.033918366, 0.07096526, -0.03316681, -0.023804352, 0.022323746, -0.015443194, 0.0048795766, 0.033774365, 0.029876191, 0.0058686067, 0.032886833, -0.06870946, 0.026777372, 0.04533471, -0.013259633, -0.010858694, -0.011202843, 0.037705757, 0.012097504, -0.01208627, 0.0030237876, -0.00743005, -0.00865434, -0.022417564, -0.026604772, -0.041180015, -0.03912838, 0.010164547, -0.0020220992, 0.025545763, -0.0012490411, -0.019990077, 0.0016675047, -0.0469453, 0.022881068, -0.037406538, 0.050526306, -0.09271968, -0.025096063, -0.006744378, -0.021286149, -0.020033577, 0.007120092, -0.026258556, -0.023049818, 0.042618018, -0.03421219, -0.0007965377, 0.042961508, 0.027561447, 0.045783397, -0.05008517, 0.014491079, -0.0008343511, 0.036284883, -0.08963851, 0.078594856, -0.02871388, -0.019006452, 0.03161922, -0.0050970553, -0.0031984644, 0.003579988, -0.0029254253, 0.034372993, -0.05684346, 0.031462703, 0.008794808, -0.032125674, -0.039078787, -0.017667467, 0.01619574, 0.00794285, -0.03460472, -0.02646594, -0.04864096, -0.012675156, 0.016825972, -0.047021937, 0.008472261, -0.034581553, 0.032546673, 0.045016345, -0.0026441524, 0.03542303, 0.010597486, -0.022957051, 0.058115974, -0.0090088, 0.01007658, -0.046193346, -0.0054717446, -0.052571986, -0.067024544, -0.007985251, 0.022404538, -0.03655897, 0.003946656, 0.015773075, 0.01893737, 0.02298624, -0.022197967, -0.01586812, -0.017752178, -0.028062796, 0.05284594, 0.013853937, 0.015723895, 0.033402957, -0.031460036, 0.012633913, -0.004863496, 0.015662119, -0.0007039063, -0.007691397, -0.022077618, 0.015838405, -0.050525676, -0.020863533, 0.024149481, -0.03744047, 0.026744612, 0.0071763974, -0.03954386, -0.031586293, 0.04133012, 0.023972139, -0.0071332185, -0.016989144, 0.017431252, -0.018055571, -0.089756556, -0.010880679, -0.03639182, -0.051707596, -0.04856756, 0.016369293, 0.0064266864, 0.038604524, -0.005459331, -0.0017621813, -0.04698625, -0.0111922035, 0.02366854, 0.005519277, -0.026277367, -0.012642915, -0.025731707, -0.013950884, 0.04487044, -0.019605804, -0.05293625, -0.010062093, -0.02069922, -0.0062642233, -0.0635588, -0.08950974, -0.0007236942, 0.016779125, 0.040270764, 0.08933929, -0.05130503, 0.057407342, -0.007821326, -0.013653576, 0.0082394015, 0.04267265, -0.038868297, 0.018410271, -0.014701536, -0.022947323, -0.00056820275, 0.0041147447, 0.0001936479, -0.00820067, 0.043621857, 0.03801962, 0.027279366, -0.07427718, 0.0046490007, 0.0074108625, -0.032219235, -0.04455989, -0.038642418, 0.024424022, 0.04309468, -0.006721593, -0.007757826, -0.034746915, -0.046649072, 0.008390074, 0.009048495, 0.014335791, 0.026588978, -0.029591406, -0.043933604, 0.024186568, -0.002524379, -0.041215096, 0.0779102, -0.060588274, -0.03840834, -0.054063305, 0.032171354, -0.010149984, 0.04051785, 0.0061978255, 0.020179631, -0.021993086, -0.026318323, 0.0041279364, -0.039625727, 0.02020406, 0.009733625, 0.02628061, -0.043017514, -0.031857546, -0.036365233, -0.034089588, -0.06729633, -0.011064906, 0.06139868, -0.0054858243, 0.10158999, 0.015421433, -0.02346649, 0.002814186, -0.05557829, -0.023346096, 0.00514023, -0.004021367, -0.0043939822, 0.03873507, -0.0028690759, 0.024786033, -0.026508927, 0.020251665, -0.014991656, 0.007782912, 0.04032214, 0.061367217, 0.069368996, 0.01091362, -0.02340269, -0.034924075, -0.038234048, 0.049985215, 0.0028436084, -0.042590823, 0.004178829, -0.009300732, 0.00033084184, 0.014607056, -0.0129903825, -0.064193755, -0.025874052, 0.0016387173, 0.00057675823, -0.028809352, -0.0492777, -0.0078082127, -0.013399179, 0.008890706, -0.05957026, 0.028899113, 0.0077516385, -0.03948989, -0.052838508, 0.010874054, 0.01153561, 0.004397799, 0.040152173, 0.008599355, 0.0067054885, 5.3130334e-5, 0.021631584, 0.016429877, -2.1479236e-5, 0.10367404, -0.01969008, 0.056486208, -0.034355808, -0.029146247, 0.00028521975, 0.04641354] }) }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let command = \"GEMINI_API_KEY=Your_API_Key ../target/release/gems -m 'embedding-001' embed -t 'hello'\";\n",
    "\n",
    "if let Err(err) = execute_command(command) {\n",
    "    eprintln!(\"Error executing command: {}\", err);\n",
    "}\n",
    "\n",
    "// Replace Your_API_Key above with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028136e",
   "metadata": {},
   "source": [
    "## 5. Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5860743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash\n",
      "    Version: 2.0\n",
      "    Display Name: Gemini 2.0 Flash\n",
      "    Description: Gemini 2.0 Flash\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let command = \"GEMINI_API_KEY=Your_API_Key ../target/release/gems info\";\n",
    "\n",
    "if let Err(err) = execute_command(command) {\n",
    "    eprintln!(\"Error executing command: {}\", err);\n",
    "}\n",
    "\n",
    "// Replace Your_API_Key above with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf7e95",
   "metadata": {},
   "source": [
    "## 6. Models Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d875ac0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n",
      "    Name: chat-bison-001\n",
      "    Version: 001\n",
      "    Display Name: PaLM 2 Chat (Legacy)\n",
      "    Description: A legacy text-only model optimized for chat conversations\n",
      "    Input Token Limit: 4096\n",
      "    Output Token Limit: 1024\n",
      "    Supported Generation Methods: [\"generateMessage\", \"countMessageTokens\"]\n",
      "    Temperature: Some(0.25)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: text-bison-001\n",
      "    Version: 001\n",
      "    Display Name: PaLM 2 (Legacy)\n",
      "    Description: A legacy model that understands text and generates text as an output\n",
      "    Input Token Limit: 8196\n",
      "    Output Token Limit: 1024\n",
      "    Supported Generation Methods: [\"generateText\", \"countTextTokens\", \"createTunedTextModel\"]\n",
      "    Temperature: Some(0.7)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: embedding-gecko-001\n",
      "    Version: 001\n",
      "    Display Name: Embedding Gecko\n",
      "    Description: Obtain a distributed representation of a text.\n",
      "    Input Token Limit: 1024\n",
      "    Output Token Limit: 1\n",
      "    Supported Generation Methods: [\"embedText\", \"countTextTokens\"]\n",
      "    Temperature: None\n",
      "    Max Temperature: None\n",
      "    Top P: None\n",
      "    Top K: None\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.0-pro-vision-latest\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.0 Pro Vision\n",
      "    Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "    Input Token Limit: 12288\n",
      "    Output Token Limit: 4096\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(0.4)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(1.0)\n",
      "    Top K: Some(32)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-pro-vision\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.0 Pro Vision\n",
      "    Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "    Input Token Limit: 12288\n",
      "    Output Token Limit: 4096\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(0.4)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(1.0)\n",
      "    Top K: Some(32)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-pro-latest\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Pro Latest\n",
      "    Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "    Input Token Limit: 2000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-pro-001\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Pro 001\n",
      "    Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "    Input Token Limit: 2000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\", \"createCachedContent\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-pro-002\n",
      "    Version: 002\n",
      "    Display Name: Gemini 1.5 Pro 002\n",
      "    Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "    Input Token Limit: 2000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\", \"createCachedContent\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-pro\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Pro\n",
      "    Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "    Input Token Limit: 2000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-latest\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash Latest\n",
      "    Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-001\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash 001\n",
      "    Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\", \"createCachedContent\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-001-tuning\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash 001 Tuning\n",
      "    Description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "    Input Token Limit: 16384\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\", \"createTunedModel\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash\n",
      "    Description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-002\n",
      "    Version: 002\n",
      "    Display Name: Gemini 1.5 Flash 002\n",
      "    Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\", \"createCachedContent\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-8b\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash-8B\n",
      "    Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"createCachedContent\", \"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-8b-001\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash-8B 001\n",
      "    Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"createCachedContent\", \"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-8b-latest\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash-8B Latest\n",
      "    Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"createCachedContent\", \"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-8b-exp-0827\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash 8B Experimental 0827\n",
      "    Description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-1.5-flash-8b-exp-0924\n",
      "    Version: 001\n",
      "    Display Name: Gemini 1.5 Flash 8B Experimental 0924\n",
      "    Description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "    Input Token Limit: 1000000\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.5-pro-exp-03-25\n",
      "    Version: 2.5-exp-03-25\n",
      "    Display Name: Gemini 2.5 Pro Experimental 03-25\n",
      "    Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 65536\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.5-pro-preview-03-25\n",
      "    Version: 2.5-preview-03-25\n",
      "    Display Name: Gemini 2.5 Pro Preview 03-25\n",
      "    Description: Gemini 2.5 Pro Preview 03-25\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 65536\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-exp\n",
      "    Version: 2.0\n",
      "    Display Name: Gemini 2.0 Flash Experimental\n",
      "    Description: Gemini 2.0 Flash Experimental\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\", \"bidiGenerateContent\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash\n",
      "    Version: 2.0\n",
      "    Display Name: Gemini 2.0 Flash\n",
      "    Description: Gemini 2.0 Flash\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-001\n",
      "    Version: 2.0\n",
      "    Display Name: Gemini 2.0 Flash 001\n",
      "    Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-exp-image-generation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Version: 2.0\n",
      "    Display Name: Gemini 2.0 Flash (Image Generation) Experimental\n",
      "    Description: Gemini 2.0 Flash (Image Generation) Experimental\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\", \"bidiGenerateContent\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-lite-001\n",
      "    Version: 2.0\n",
      "    Display Name: Gemini 2.0 Flash-Lite 001\n",
      "    Description: Stable version of Gemini 2.0 Flash Lite\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-lite\n",
      "    Version: 2.0\n",
      "    Display Name: Gemini 2.0 Flash-Lite\n",
      "    Description: Gemini 2.0 Flash-Lite\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-lite-preview-02-05\n",
      "    Version: preview-02-05\n",
      "    Display Name: Gemini 2.0 Flash-Lite Preview 02-05\n",
      "    Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-lite-preview\n",
      "    Version: preview-02-05\n",
      "    Display Name: Gemini 2.0 Flash-Lite Preview\n",
      "    Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-pro-exp\n",
      "    Version: 2.5-exp-03-25\n",
      "    Display Name: Gemini 2.0 Pro Experimental\n",
      "    Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 65536\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-pro-exp-02-05\n",
      "    Version: 2.5-exp-03-25\n",
      "    Display Name: Gemini 2.0 Pro Experimental 02-05\n",
      "    Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 65536\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-exp-1206\n",
      "    Version: 2.5-exp-03-25\n",
      "    Display Name: Gemini Experimental 1206\n",
      "    Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 65536\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-thinking-exp-01-21\n",
      "    Version: 2.0-exp-01-21\n",
      "    Display Name: Gemini 2.0 Flash Thinking Experimental 01-21\n",
      "    Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 65536\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(0.7)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-thinking-exp\n",
      "    Version: 2.0-exp-01-21\n",
      "    Display Name: Gemini 2.0 Flash Thinking Experimental 01-21\n",
      "    Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 65536\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(0.7)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-thinking-exp-1219\n",
      "    Version: 2.0\n",
      "    Display Name: Gemini 2.0 Flash Thinking Experimental\n",
      "    Description: Gemini 2.0 Flash Thinking Experimental\n",
      "    Input Token Limit: 1048576\n",
      "    Output Token Limit: 65536\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(0.7)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: learnlm-1.5-pro-experimental\n",
      "    Version: 001\n",
      "    Display Name: LearnLM 1.5 Pro Experimental\n",
      "    Description: Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "    Input Token Limit: 32767\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemma-3-1b-it\n",
      "    Version: 001\n",
      "    Display Name: Gemma 3 1B\n",
      "    Description: N/A\n",
      "    Input Token Limit: 32768\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemma-3-4b-it\n",
      "    Version: 001\n",
      "    Display Name: Gemma 3 4B\n",
      "    Description: N/A\n",
      "    Input Token Limit: 32768\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemma-3-12b-it\n",
      "    Version: 001\n",
      "    Display Name: Gemma 3 12B\n",
      "    Description: N/A\n",
      "    Input Token Limit: 32768\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemma-3-27b-it\n",
      "    Version: 001\n",
      "    Display Name: Gemma 3 27B\n",
      "    Description: N/A\n",
      "    Input Token Limit: 131072\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"generateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: embedding-001\n",
      "    Version: 001\n",
      "    Display Name: Embedding 001\n",
      "    Description: Obtain a distributed representation of a text.\n",
      "    Input Token Limit: 2048\n",
      "    Output Token Limit: 1\n",
      "    Supported Generation Methods: [\"embedContent\"]\n",
      "    Temperature: None\n",
      "    Max Temperature: None\n",
      "    Top P: None\n",
      "    Top K: None\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: text-embedding-004\n",
      "    Version: 004\n",
      "    Display Name: Text Embedding 004\n",
      "    Description: Obtain a distributed representation of a text.\n",
      "    Input Token Limit: 2048\n",
      "    Output Token Limit: 1\n",
      "    Supported Generation Methods: [\"embedContent\"]\n",
      "    Temperature: None\n",
      "    Max Temperature: None\n",
      "    Top P: None\n",
      "    Top K: None\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-embedding-exp-03-07\n",
      "    Version: exp-03-07\n",
      "    Display Name: Gemini Embedding Experimental 03-07\n",
      "    Description: Obtain a distributed representation of a text.\n",
      "    Input Token Limit: 8192\n",
      "    Output Token Limit: 1\n",
      "    Supported Generation Methods: [\"embedContent\", \"countTextTokens\"]\n",
      "    Temperature: None\n",
      "    Max Temperature: None\n",
      "    Top P: None\n",
      "    Top K: None\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-embedding-exp\n",
      "    Version: exp-03-07\n",
      "    Display Name: Gemini Embedding Experimental\n",
      "    Description: Obtain a distributed representation of a text.\n",
      "    Input Token Limit: 8192\n",
      "    Output Token Limit: 1\n",
      "    Supported Generation Methods: [\"embedContent\", \"countTextTokens\"]\n",
      "    Temperature: None\n",
      "    Max Temperature: None\n",
      "    Top P: None\n",
      "    Top K: None\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: aqa\n",
      "    Version: 001\n",
      "    Display Name: Model that performs Attributed Question Answering.\n",
      "    Description: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
      "    Input Token Limit: 7168\n",
      "    Output Token Limit: 1024\n",
      "    Supported Generation Methods: [\"generateAnswer\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Temperature: Some(0.2)\n",
      "    Max Temperature: None\n",
      "    Top P: Some(1.0)\n",
      "    Top K: Some(40)\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: imagen-3.0-generate-002\n",
      "    Version: 002\n",
      "    Display Name: Imagen 3.0 002 model\n",
      "    Description: Vertex served Imagen 3.0 002 model\n",
      "    Input Token Limit: 480\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"predict\"]\n",
      "    Temperature: None\n",
      "    Max Temperature: None\n",
      "    Top P: None\n",
      "    Top K: None\n",
      "\n",
      "\n",
      "Model:\n",
      "    Name: gemini-2.0-flash-live-001\n",
      "    Version: 001\n",
      "    Display Name: Gemini 2.0 Flash 001\n",
      "    Description: Gemini 2.0 Flash 001\n",
      "    Input Token Limit: 131072\n",
      "    Output Token Limit: 8192\n",
      "    Supported Generation Methods: [\"bidiGenerateContent\", \"countTokens\"]\n",
      "    Temperature: Some(1.0)\n",
      "    Max Temperature: Some(2.0)\n",
      "    Top P: Some(0.95)\n",
      "    Top K: Some(64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let command = \"GEMINI_API_KEY=Your_API_Key ../target/release/gems list\";\n",
    "\n",
    "if let Err(err) = execute_command(command) {\n",
    "    eprintln!(\"Error executing command: {}\", err);\n",
    "}\n",
    "\n",
    "// Replace Your_API_Key above with your actual API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab87b8f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
